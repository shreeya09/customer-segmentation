{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreeya09/customer-segmentation/blob/main/Unsupervised_ML_Myntra_Online_Retail_Customer_Segmentation_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Myntra is a leading Indian fashion e-commerce company known for its diverse collection of clothing, accessories, and lifestyle products. While predominantly associated with fashion retail in India, this project focuses on a dataset from Myntra Gifts Ltd., a UK-based division that deals in unique all-occasion giftware. The dataset includes detailed records of online retail transactions made through the company’s non-store platform between December 1, 2009, and December 9, 2011. It offers a rich snapshot of international retail activity, capturing customer purchases, geographic information, product details, and pricing.\n",
        "\n",
        "The main objective of this project is to perform customer segmentation using unsupervised machine learning techniques, helping the business uncover patterns and optimize strategies across various domains. With no predefined labels or categories, clustering algorithms such as K-Means are used to group customers based on their purchasing behavior, frequency, monetary value, and more. The result is a deeper understanding of customer profiles, which can guide marketing, inventory, and pricing decisions.\n",
        "\n",
        "Key Goals of the Project:\n",
        "\n",
        "1. Identifying Purchasing Trends:\n",
        "By analyzing the time and frequency of purchases, this project aims to uncover seasonal patterns, popular months, and preferred product types. These insights help in planning marketing campaigns and aligning inventory with demand cycles.\n",
        "\n",
        "2. Evaluating Product Performance:\n",
        "The data allows for a detailed analysis of which products are selling well and which are underperforming. Understanding top-selling categories and SKUs enables smarter inventory stocking and targeted promotions.\n",
        "\n",
        "3. Understanding Customer Behavior:\n",
        "Segmenting customers based on metrics like Recency, Frequency, and Monetary value (RFM) helps identify loyal customers, occasional buyers, and high-value clients. This allows for better personalization and customer retention strategies.\n",
        "\n",
        "4. Optimizing Pricing Strategies:\n",
        "Exploring the link between unit prices and sales volumes helps the business find ideal pricing points that maximize revenue without losing competitiveness.\n",
        "\n",
        "5. Streamlining Inventory Management:\n",
        "With insights into sales trends and customer demand, the company can reduce overstock and stockouts, ensuring better inventory turnover and improved customer satisfaction."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "18n_sEyLXSKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github link:"
      ],
      "metadata": {
        "id": "ItZLFR-AXbaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Myntra Gifts Ltd., a UK-based division of Myntra specializing in all-occasion giftware, has accumulated a large volume of online retail transaction data from 2009 to 2011. However, the company lacks a structured understanding of its customer base, purchasing patterns, and product performance. Without clear segmentation, it is challenging to implement targeted marketing, optimize inventory, or personalize customer experiences.\n",
        "\n",
        "The goal of this project is to apply unsupervised machine learning techniques to segment customers based on their transactional behavior. By identifying distinct customer groups, the company can develop data-driven strategies to improve customer retention, streamline operations, and enhance overall business performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing and clustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Load the dataset\n",
        "file_path = 'https://raw.githubusercontent.com/shreeya09/customer-segmentation/main/Online%20Retail.xlsx'\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "# Shape of the dataset\n",
        "rows, columns = df.shape\n",
        "print(f\"Number of rows: {rows}\")\n",
        "print(f\"Number of columns: {columns}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing/Null values in each column:\\n\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 541909 rows and 8 columns .\n",
        "\n",
        "It includes transaction-level retail data such as:\n",
        "\n",
        "**-InvoiceNo**\n",
        "\n",
        "**-StockCode**\n",
        "\n",
        "**-Description**\n",
        "\n",
        "**-Quantity**\n",
        "\n",
        "**-InvoiceDate**\n",
        "\n",
        "**-UnitPrice**\n",
        "\n",
        "**-CustomerID**\n",
        "\n",
        "**-Country**\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "**-Duplicate Rows:**\n",
        "\n",
        "There are some duplicate entries, which may need removal to avoid skewing analysis.\n",
        "\n",
        "**-Missing Values:**\n",
        "\n",
        "Significant missing values exist in columns like CustomerID and Description, commonly seen in e-commerce transaction data.\n",
        "\n",
        "These must be handled carefully—either filled, dropped, or used for imputation depending on the context.\n",
        "\n",
        "**-Data Types:**\n",
        "\n",
        "Numeric: Quantity, UnitPrice\n",
        "\n",
        "Categorical: InvoiceNo, StockCode, Description, Country\n",
        "\n",
        "DateTime: InvoiceDate\n",
        "\n",
        "Identifier: CustomerID (though has missing values)\n",
        "\n",
        "**-Transaction Granularity:**\n",
        "\n",
        "Each row represents a line item in a transaction (not full orders).\n",
        "\n",
        "**-Country:**\n",
        "\n",
        "Though it’s from Myntra Gifts Ltd. in the UK, it contains transactions from multiple countries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Dataset Columns:\\n\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**InvoiceNo**:\tUnique identifier for each transaction. If it starts with \"C\", it indicates a cancellation.\n",
        "\n",
        "**StockCode**:\tUnique code for each product/item.\n",
        "\n",
        "**Description**:\tName/description of the product.\n",
        "\n",
        "**Quantity**:\tNumber of items purchased per transaction line. Can be negative for returns.\n",
        "\n",
        "**InvoiceDate**:\tTimestamp of the transaction (date and time).\n",
        "\n",
        "**UnitPrice**:\tPrice per unit of the product (in GBP).\n",
        "\n",
        "**CustomerID**:\tUnique identifier for the customer. Missing values may indicate guest or unregistered purchases.\n",
        "\n",
        "**Country**:\tCountry where the customer is located."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = df.nunique()\n",
        "print(\"Unique values in each column:\\n\")\n",
        "print(unique_values)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# --- Initial shape ---\n",
        "print(f\"Initial dataset shape: {df.shape}\")\n",
        "\n",
        "# --- Remove duplicate rows ---\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"After removing duplicates: {df.shape}\")\n",
        "\n",
        "# --- Handle missing values ---\n",
        "# We'll drop rows with missing CustomerID, since clustering on customers is likely part of unsupervised ML\n",
        "df = df.dropna(subset=['CustomerID'])\n",
        "\n",
        "# For Description, we can fill missing with 'Unknown' (to retain data integrity)\n",
        "df['Description'] = df['Description'].fillna('Unknown')\n",
        "\n",
        "# --- Filter invalid transactions ---\n",
        "# Remove rows with negative or zero Quantity or UnitPrice\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "\n",
        "# --- Convert date columns ---\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# --- Add TotalPrice column ---\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# --- Reset index ---\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# --- Data types check ---\n",
        "print(\"\\nData types after wrangling:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# --- Basic stats ---\n",
        "print(\"\\nCleaned dataset shape:\", df.shape)\n",
        "print(\"\\nMissing values (after cleaning):\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Removed duplicate rows\n",
        "\n",
        "-Duplicates can mislead clustering algorithms and inflate customer behavior metrics.\n",
        "\n",
        "-Handled missing values\n",
        "\n",
        "-Dropped rows with missing CustomerID: These cannot be used in customer segmentation.\n",
        "\n",
        "-Filled missing Description with 'Unknown': Allows retention of the data while acknowledging the lack of item detail.\n",
        "\n",
        "-Filtered invalid transactions\n",
        "\n",
        "-Removed rows with non-positive Quantity or UnitPrice: Such entries are often returns, errors, or test data that can distort clustering.\n",
        "\n",
        "-Converted InvoiceDate to datetime format for future time-based analysis (e.g., recency).\n",
        "\n",
        "-Created a TotalPrice column. Helps in computing monetary value per transaction/customer. TotalPrice = Quantity × UnitPrice\n",
        "\n",
        "-Reset index after all row deletions and changes for a clean dataframe structure."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1- Top Countries by Transaction Volume (Barplot)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "top_countries = df['Country'].value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_countries.values, y=top_countries.index, palette='viridis')\n",
        "plt.title('Top 10 Countries by Number of Transactions')\n",
        "plt.xlabel('Transaction Count')\n",
        "plt.ylabel('Country')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart is best for ranked comparisons. We’re examining categorical values (countries), and bar charts clearly show volumes across distinct groups."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "United Kingdom dominates the transaction volume.\n",
        "\n",
        "Most other transactions are concentrated in a few EU countries (Netherlands, Germany, France, etc.).\n",
        "\n",
        "Some countries have very few transactions, indicating sparse or one-time activity.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Impact:**\n",
        "Positive: Helps target marketing and resource allocation in high-engagement countries.\n",
        "\n",
        "Country-specific promotions can be tailored for top markets.\n",
        "\n",
        "\n",
        "**Negative Growth Insight:**\n",
        "If a country has high returns or low spending, even with many transactions, it may signal low profitability.\n",
        "\n",
        "Some countries may only make bulk purchases rarely — not ideal for customer retention.\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2- Top 10 Products by Quantity Sold (Bar Chart)"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_products.values, y=top_products.index, palette='rocket')\n",
        "plt.title('Top 10 Products by Quantity Sold')\n",
        "plt.xlabel('Total Quantity Sold')\n",
        "plt.ylabel('Product Description')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart ranks products by popularity. Quantity sold is a volume measure, and this chart shows bestsellers clearly."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some items dominate the quantity sold (e.g., party accessories, small home goods).\n",
        "\n",
        "These might be low-margin but high-volume items."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Impact:**\n",
        "\n",
        "Positive: Helps in optimizing inventory and predicting demand.\n",
        "\n",
        "Can focus on bundling, upselling these products.\n",
        "\n",
        "\n",
        "**Negative Growth Insight:**\n",
        "\n",
        "If these top-selling products have low profit margins, they may not contribute much to revenue.\n",
        "\n",
        "Could indicate over-reliance on a few products — diversification may be necessary.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3- Monthly Sales Trend(Line Chart)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "df['Month'] = df['InvoiceDate'].dt.to_period('M').astype(str)\n",
        "monthly_sales = df.groupby('Month')['TotalPrice'].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.lineplot(data=monthly_sales, x='Month', y='TotalPrice', marker='o')\n",
        "plt.title('Monthly Sales Trend')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Total Sales (£)')\n",
        "plt.xlabel('Month')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are ideal for time series data to show trends over time — here, monthly revenue (TotalPrice)."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seasonality is clear: sales increase toward November–December (holiday season).\n",
        "\n",
        "Possible dip in summer months or early year."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Impact:**\n",
        "Positive: Plan stock, staffing, and campaigns for peak seasons.\n",
        "\n",
        "Helps forecast demand and optimize logistics.\n",
        "\n",
        "**Negative Growth Insight:**\n",
        "Large off-season dips in revenue indicate overdependence on festive sales.\n",
        "\n",
        "Suggests need for year-round engagement strategies."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4- Invoice Size Distribution (Histogram)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "invoice_value = df.groupby('InvoiceNo')['TotalPrice'].sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(invoice_value, bins=100, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Invoice Value')\n",
        "plt.xlabel('Total Invoice Value (£)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 2000)  # zooming in to remove extreme outliers\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram with KDE (kernel density) shows the distribution of numerical values (Total invoice value)."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most invoices are clustered below £500.\n",
        "\n",
        "Few high-value transactions (long tail) — might be wholesale buyers or anomalies."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Impact:**\n",
        "Positive: Understand what’s a “typical” order size.\n",
        "\n",
        "Enables pricing strategy or tiered offers (e.g., free shipping over £200).\n",
        "\n",
        "**Negative Growth Insight:**\n",
        "Over-reliance on small purchases may limit revenue growth.\n",
        "\n",
        "Need to incentivize higher cart values (bundles, discounts)."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5- Unit Price vs Quantity (Scatterplot)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df[df['Quantity'] < 1000], x='UnitPrice', y='Quantity', alpha=0.5)\n",
        "plt.title('Scatter: Unit Price vs Quantity')\n",
        "plt.xlabel('Unit Price (£)')\n",
        "plt.ylabel('Quantity')\n",
        "plt.xscale('log')  # log scale to compress outliers\n",
        "plt.yscale('log')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots help reveal relationships and outliers between two continuous variables (price vs quantity)."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "High quantity usually correlates with low price (bulk buying behavior).\n",
        "\n",
        "Outliers like very high-priced items with low quantities also exist."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Impact:**\n",
        "Positive: Helps identify bulk buyers or products that sell well at scale.\n",
        "\n",
        "Segment users based on high-volume, low-price behavior.\n",
        "\n",
        "**Negative Growth Insight:**\n",
        "If only cheap items sell in volume, high-value products may be underperforming.\n",
        "\n",
        "Indicates a price sensitivity problem in customer base or poor luxury positioning."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Select only numerical columns for correlation\n",
        "num_cols = ['Quantity', 'UnitPrice', 'TotalPrice']\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df[num_cols].corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap visually displays the strength of linear relationships between numerical features. It's fast, compact, and immediately shows which variables move together."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap of the retail dataset reveals important linear relationships between key numerical variables. Most notably, there is a strong positive correlation between Quantity and TotalPrice, indicating that as more items are purchased, the total transaction value increases — a natural but crucial validation for revenue generation. There is also a moderate positive correlation between UnitPrice and TotalPrice, suggesting that higher-priced products contribute to larger invoice values, although not as strongly as quantity does. Interestingly, the correlation between Quantity and UnitPrice appears to be weak or slightly negative, which implies that bulk purchases are typically associated with lower-priced items. These insights highlight that the business’s revenue is more heavily driven by volume than pricing, and while this supports strong sales figures, it also raises potential concerns about over-dependence on low-margin products. Overall, the heatmap provides foundational understanding for feature selection and sets the stage for more advanced segmentation and clustering."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "# Sample a subset to avoid overplotting (optional but recommended for performance)\n",
        "df_sampled = df[['Quantity', 'UnitPrice', 'TotalPrice']].sample(1000, random_state=42)\n",
        "\n",
        "# Create pair plot\n",
        "sns.pairplot(df_sampled, diag_kind='kde', corner=True)\n",
        "plt.suptitle('Pair Plot of Numerical Features (Sampled)', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is chosen to:\n",
        "\n",
        "-Visually inspect relationships between numerical features\n",
        "\n",
        "-See the distribution of each feature (histograms/KDE)\n",
        "\n",
        "-Detect non-linear patterns, outliers, or natural clusters before applying unsupervised learning"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Clear positive relationship between Quantity & TotalPrice (visible upward trend)\n",
        "\n",
        "-Spread in UnitPrice shows some skew — most items are low-cost, a few are premium-priced\n",
        "\n",
        "-Clusters or groupings may appear in TotalPrice distribution → hinting at segments\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1**: Customers from the UK spend more per transaction than customers from other countries.\n",
        "Why: UK dominates transaction volume — but do they also bring higher value?\n",
        "Test: Two-sample t-test (UK vs Non-UK on average TotalPrice)\n",
        "\n",
        "**Hypothesis 2**:\n",
        "High unit price items are purchased in smaller quantities than low unit price items.\n",
        "Why: Visuals show bulk items are low-cost; let’s test if high prices suppress quantity.\n",
        "Test: Pearson correlation test between UnitPrice and Quantity\n",
        "\n",
        "**Hypothesis 3**:\n",
        "The average invoice value is significantly higher during the holiday season (Nov–Dec) compared to other months.\n",
        "Why: Monthly sales chart suggests seasonal spikes — but is the invoice size also higher?\n",
        "Test: Two-sample t-test (Invoice value in Nov–Dec vs. other months)"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): Average TotalPrice of UK transactions ≤ Non-UK transactions\n",
        "\n",
        "Alternate Hypothesis (H₁): Average TotalPrice of UK transactions > Non-UK transactions\n",
        "\n",
        "Test Used: Independent t-test (one-tailed)"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import ttest_ind, pearsonr\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Hypothesis 1: UK vs Non-UK TotalPrice comparison\n",
        "# ---------------------------------------------------\n",
        "uk_total = df[df['Country'] == 'United Kingdom']['TotalPrice']\n",
        "non_uk_total = df[df['Country'] != 'United Kingdom']['TotalPrice']\n",
        "\n",
        "t_stat_1, p_val_1 = ttest_ind(uk_total, non_uk_total, equal_var=False, alternative='greater')\n",
        "\n",
        "# Print results\n",
        "print(\"Hypothesis 1 (UK > Non-UK Spending):\")\n",
        "print(f\"T-statistic = {t_stat_1:.2f}, P-value = {p_val_1:.4f}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Independent Two-Sample t-test (one-tailed)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why this test was chosen:**\n",
        "\n",
        "We are comparing the means of a numerical variable (TotalPrice) across two independent groups: UK vs Non-UK.\n",
        "\n",
        "The two-sample t-test is appropriate when:\n",
        "\n",
        "You want to compare means between two groups\n",
        "\n",
        "The groups are independent (not related)\n",
        "\n",
        "The variable is continuous and approximately normally distributed (or large sample size for CLT to hold)\n",
        "\n",
        "A one-tailed test was used because we’re specifically testing if UK > Non-UK in spending."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): No correlation between UnitPrice and Quantity\n",
        "\n",
        "Alternate Hypothesis (H₁): Significant negative correlation exists\n",
        "\n",
        "Test Used: Pearson correlation"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# ---------------------------------------------------\n",
        "# Hypothesis 2: Correlation between UnitPrice and Quantity\n",
        "# ---------------------------------------------------\n",
        "unit_price = df['UnitPrice']\n",
        "quantity = df['Quantity']\n",
        "\n",
        "corr_coef_2, p_val_2 = pearsonr(unit_price, quantity)\n",
        "\n",
        "print(\"Hypothesis 2 (UnitPrice vs Quantity Correlation):\")\n",
        "print(f\"Correlation Coefficient = {corr_coef_2:.4f}, P-value = {p_val_2:.4f}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Pearson Correlation Coefficient Test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why this test was chosen:**\n",
        "\n",
        "We’re testing for a linear relationship between two continuous numeric variables: UnitPrice and Quantity.\n",
        "\n",
        "The Pearson correlation quantifies the strength and direction of the linear association.\n",
        "\n",
        "It also gives a p-value to determine if the correlation is statistically significant."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀): Holiday season invoice value ≤ Rest of the year\n",
        "\n",
        "Alternate Hypothesis (H₁): Holiday season invoice value > Rest of the year\n",
        "\n",
        "Test Used: Independent t-test (one-tailed)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# ---------------------------------------------------\n",
        "# Hypothesis 3: Holiday (Nov–Dec) vs other months\n",
        "# ---------------------------------------------------\n",
        "df['Month'] = df['InvoiceDate'].dt.month\n",
        "holiday_df = df[df['Month'].isin([11, 12])]['TotalPrice']\n",
        "non_holiday_df = df[~df['Month'].isin([11, 12])]['TotalPrice']\n",
        "\n",
        "t_stat_3, p_val_3 = ttest_ind(holiday_df, non_holiday_df, equal_var=False, alternative='greater')\n",
        "\n",
        "print(\"Hypothesis 3 (Holiday Season Spending):\")\n",
        "print(f\"T-statistic = {t_stat_3:.2f}, P-value = {p_val_3:.4f}\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Test Used: Independent Two-Sample t-test (one-tailed)"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why this test was chosen:**\n",
        "\n",
        "Similar to Hypothesis 1, we’re comparing means of TotalPrice between two groups:\n",
        "\n",
        "Transactions during Nov–Dec (holiday)\n",
        "\n",
        "Transactions in the rest of the year (non-holiday)\n",
        "\n",
        "These two groups are independent, and the variable is continuous.\n",
        "\n",
        "A one-tailed t-test was chosen because the hypothesis specifically predicts higher values during the holiday season."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many missing values in each column\n",
        "# Drop rows with missing CustomerID\n",
        "df = df.dropna(subset=['CustomerID'])\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop rows with missing values used. Since our segmentation is customer-based, CustomerID is essential. Rows without it can't be used."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot boxplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.boxplot(x=df['Quantity'], ax=axes[0])\n",
        "axes[0].set_title('Boxplot of Quantity')\n",
        "\n",
        "sns.boxplot(x=df['UnitPrice'], ax=axes[1])\n",
        "axes[1].set_title('Boxplot of UnitPrice')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Remove negative or zero values\n",
        "df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
        "\n",
        "# Create total price column\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Capping extreme values using 99th percentile\n",
        "q99_quantity = df['Quantity'].quantile(0.99)\n",
        "q99_unitprice = df['UnitPrice'].quantile(0.99)\n",
        "\n",
        "df = df[(df['Quantity'] <= q99_quantity) & (df['UnitPrice'] <= q99_unitprice)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We addressed outliers using a combination of domain-driven and statistical techniques to ensure data quality and reliable insights. We began by removing invalid entries, such as rows with zero or negative values in the `Quantity` and `UnitPrice` columns, as these typically represent returns, cancellations, or data entry errors and do not reflect actual purchases. To further reduce the impact of extreme values without significantly affecting the dataset's volume, we applied capping using the 99th percentile for both `Quantity` and `UnitPrice`. This method preserved the majority of the data while minimizing skew from unusually large transactions. Additionally, we created a new feature, `TotalPrice`, by multiplying `Quantity` and `UnitPrice`, which will be crucial in calculating monetary value for customer segmentation. This approach ensured that the data remained business-relevant and suitable for clustering-based segmentation."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encode the 'Country' column\n",
        "df_encoded = pd.get_dummies(df, columns=['Country'], drop_first=True)\n",
        "\n",
        "# Display the first few rows to confirm encoding\n",
        "df_encoded.head()\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding\n",
        "\n",
        "Applied To: Country (the main categorical feature used for modeling)\n",
        "\n",
        "What it does:\n",
        "Converts each unique category into a separate binary column (0/1).\n",
        "\n",
        "Why this technique?\n",
        "\n",
        "Suitable for non-ordinal categorical variables.\n",
        "\n",
        "Ensures no false assumptions about order or distance between categories.\n",
        "\n",
        "Works well with distance-based unsupervised learning algorithms (e.g., K-Means, Hierarchical Clustering)."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords if needed\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#Convert to lowercase\n",
        "df['Description_clean'] = df['Description'].astype(str).str.lower()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation and special characters\n",
        "df['Description_clean'] = df['Description_clean'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "df['Description_clean'] = df['Description_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "df['Description_clean'] = df['Description_clean'].str.strip()"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Remove words/tokens containing digits\n",
        "df['Description_clean'] = df['Description_clean'].apply(lambda x: re.sub(r'\\w*\\d\\w*', '', x))\n",
        "\n",
        "\n",
        "# Preview cleaned descriptions\n",
        "df[['Description', 'Description_clean']].head()\n",
        "df.columns.to_list()"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TotalPrice\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Convert InvoiceDate to datetime\n",
        "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "\n",
        "# Set reference date for Recency calculation\n",
        "import datetime as dt\n",
        "reference_date = df['InvoiceDate'].max() + dt.timedelta(days=1)\n",
        "\n",
        "# RFM Feature Creation\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency\n",
        "    'InvoiceNo': 'nunique',                                     # Frequency\n",
        "    'TotalPrice': 'sum'                                         # Monetary\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns\n",
        "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
        "\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# Keep only the features needed for clustering\n",
        "rfm_selected = rfm[['Recency', 'Frequency', 'Monetary']]\n",
        "\n",
        "# Preview the selected features\n",
        "rfm_selected.head()\n",
        "\n",
        "df.columns.to_list()\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used manual feature selection based on domain knowledge rather than statistical or automated methods. Since the project focuses on customer segmentation using RFM (Recency, Frequency, Monetary) analysis, we specifically selected the features that are known to be directly relevant for measuring customer behavior. This approach ensures that the clustering model is built on meaningful and interpretable customer metrics, making the segmentation results more actionable for business decisions."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most important features identified for this analysis were:\n",
        "\n",
        "Recency: Measures the number of days since a customer’s last purchase. It helps identify how recently a customer engaged with the business.\n",
        "\n",
        "Frequency: Indicates how often a customer has purchased. This is useful for identifying loyal or repeat customers.\n",
        "\n",
        "Monetary: Represents the total spending of each customer. It helps in recognizing high-value customers who contribute more revenue.\n",
        "\n",
        "These three features effectively summarize customer behavior and are essential for building meaningful customer segments. Other features like InvoiceNo, StockCode, Description, and Country were not included, as they do not provide direct insights into purchasing patterns at the customer level."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LztYCnqim5qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the selected RFM features\n",
        "rfm_scaled = scaler.fit_transform(rfm_selected)\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=rfm_selected.columns)\n",
        "\n",
        "# Display first few rows\n",
        "rfm_scaled_df.head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "We used StandardScaler for standardization of the RFM features. This transformation converts the features to a standard normal distribution (mean = 0, standard deviation = 1). It’s ideal for clustering because it maintains the distribution of the data while ensuring that all features contribute equally to the distance calculations.\n",
        "\n",
        "Standardization was chosen over normalization because it is more robust when the data contains outliers — particularly relevant in our case, since Monetary and Frequency often have skewed distributions."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is the process of reducing the number of input variables (features) while retaining as much information as possible. It helps in:\n",
        "\n",
        "Visualizing high-dimensional data (especially for clustering)\n",
        "\n",
        "Reducing noise and redundancy\n",
        "\n",
        "Improving computational efficiency\n",
        "\n",
        "Avoiding the curse of dimensionality\n",
        "\n",
        "In our case, we’re working with only 3 features (Recency, Frequency, Monetary), so dimensionality reduction isn’t necessary for modeling, but it’s very useful for 2D visualization of clusters."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Apply PCA to reduce to 2 dimensions for visualization\n",
        "pca = PCA(n_components=2)\n",
        "rfm_pca = pca.fit_transform(rfm_scaled_df)\n",
        "\n",
        "# Convert to DataFrame\n",
        "rfm_pca_df = pd.DataFrame(rfm_pca, columns=['PCA1', 'PCA2'])\n",
        "\n",
        "# Preview the reduced data\n",
        "rfm_pca_df.head()\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used Principal Component Analysis (PCA) as the dimensionality reduction technique. PCA was chosen because it is one of the most widely used and efficient methods for reducing high-dimensional data into fewer dimensions while retaining most of the variance (information) in the data.\n",
        "\n",
        "Although our dataset only has three features (Recency, Frequency, and Monetary), we applied PCA to reduce it to two dimensions for visualization purposes. This helps us easily interpret and visualize customer clusters in a 2D scatter plot, making the segmentation results more intuitive and understandable. PCA also ensures that the axes (principal components) are orthogonal and capture the maximum variance in the data, which makes it ideal for this task."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation: K-Means Clustering\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Fit the Algorithm\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(rfm_scaled_df)\n",
        "\n",
        "# Predict on the model\n",
        "rfm_scaled_df['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Display the first few rows with cluster labels\n",
        "rfm_scaled_df.head()\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Finding optimal number of clusters using Silhouette Score\n",
        "silhouette_scores = []\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(rfm_scaled_df)\n",
        "    score = silhouette_score(rfm_scaled_df, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--')\n",
        "plt.title(\"Silhouette Score vs Number of Clusters (KMeans)\")\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different values of init method and max_iter\n",
        "params = {\n",
        "    \"init\": [\"k-means++\", \"random\"],\n",
        "    \"max_iter\": [100, 300, 500]\n",
        "}\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "\n",
        "for init in params[\"init\"]:\n",
        "    for max_iter in params[\"max_iter\"]:\n",
        "        kmeans = KMeans(n_clusters=4, init=init, max_iter=max_iter, random_state=42)\n",
        "        labels = kmeans.fit_predict(rfm_scaled_df)\n",
        "        score = silhouette_score(rfm_scaled_df, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = {\"init\": init, \"max_iter\": max_iter}\n",
        "\n",
        "print(\"Best Params:\", best_params)\n",
        "print(\"Improved Silhouette Score:\", round(best_score, 3))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search (manual) was used to tune init and max_iter because the search space is small and intuitive for KMeans."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning, the silhouette score improved by about 0.02 points — indicating more cohesive clusters.\n",
        "\n",
        "Silhouette Score: Indicates how well-separated clusters are. A higher score shows clear customer segments.\n",
        "\n",
        "Business Impact: Enables Myntra to target clusters differently — for example, identifying high-value vs low-frequency buyers and personalizing campaigns accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dendrogram to visualize optimal number of clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram = sch.dendrogram(sch.linkage(rfm_scaled_df, method='ward'))\n",
        "plt.title(\"Dendrogram (to determine optimal clusters)\")\n",
        "plt.xlabel(\"Customers\")\n",
        "plt.ylabel(\"Euclidean distances\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluate model performance for different cluster sizes\n",
        "silhouette_scores = []\n",
        "for k in range(2, 7):\n",
        "    hc = AgglomerativeClustering(n_clusters=k, linkage='ward', metric='euclidean')\n",
        "    labels = hc.fit_predict(rfm_scaled_df)\n",
        "    score = silhouette_score(rfm_scaled_df, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(2, 7), silhouette_scores, marker='o', linestyle='--', color='purple')\n",
        "plt.title('Silhouette Score vs Number of Clusters (Hierarchical Clustering)')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
        "k = 4  # Assuming 4 is optimal based on Step 1\n",
        "\n",
        "print(\"Linkage Method Tuning:\")\n",
        "for method in linkage_methods:\n",
        "    # 'ward' linkage requires 'euclidean' distance\n",
        "    if method == 'ward':\n",
        "        hc = AgglomerativeClustering(n_clusters=k, linkage=method, metric='euclidean')\n",
        "    else:\n",
        "        hc = AgglomerativeClustering(n_clusters=k, linkage=method, metric='euclidean')\n",
        "\n",
        "    labels = hc.fit_predict(rfm_scaled_df)\n",
        "    score = silhouette_score(rfm_scaled_df, labels)\n",
        "    print(f\"Linkage: {method:8} → Silhouette Score: {score:.3f}\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hierarchical Clustering, we performed a manual grid search by tuning the parameters:\n",
        "\n",
        "n_clusters (number of clusters)\n",
        "\n",
        "linkage method (ward, complete, average)\n",
        "\n",
        "We did this because:\n",
        "\n",
        "Hierarchical clustering doesn’t require initialization or iterations like KMeans.\n",
        "\n",
        "The effect of different linkage methods can significantly change cluster structure, so visualizing dendrograms and measuring silhouette scores helped pick the best fit.\n",
        "\n",
        "Manual tuning was sufficient given the small hyperparameter space."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. After tuning:\n",
        "\n",
        "Choosing linkage='ward' with n_clusters=4 gave the highest silhouette score, showing clear and balanced clusters.\n",
        "\n",
        "Improvement was noticeable from around 0.31 to 0.38 in silhouette score.\n",
        "\n",
        "This indicates that the final clusters were more well-separated and meaningful."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette Score**\n",
        "\n",
        "Measures how close a point is to its own cluster vs other clusters.\n",
        "\n",
        "Values range from -1 to 1; higher is better.\n",
        "\n",
        "In business:\n",
        "A high silhouette score indicates strong customer segmentation — making it easier to build targeted marketing strategies for each cluster.\n",
        "\n",
        "**Business Impact**\n",
        "\n",
        "Agglomerative clustering is particularly good for visualizing hierarchical relationships between customers.\n",
        "\n",
        "Myntra can use this to:\n",
        "\n",
        "Identify nested segments (e.g., frequent buyers within a high-value group).\n",
        "\n",
        "Tailor loyalty programs or personalized campaigns.\n",
        "\n",
        "Strategize tiered marketing efforts based on how customers are related in the purchase hierarchy.\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "eps_values = [0.5, 1.0, 1.5, 2.0]\n",
        "min_samples_values = [3, 5, 7]\n",
        "\n",
        "heatmap_scores = []\n",
        "\n",
        "for min_samples in min_samples_values:\n",
        "    row_scores = []\n",
        "    for eps in eps_values:\n",
        "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = db.fit_predict(rfm_scaled_df)\n",
        "\n",
        "        # Ensure we have valid clusters (no -1 labels only)\n",
        "        if len(set(labels)) > 1 and len(set(labels)) != 1 + (1 if -1 in labels else 0):\n",
        "            mask = labels != -1\n",
        "            score = silhouette_score(rfm_scaled_df[mask], labels[mask])\n",
        "        else:\n",
        "            score = -1  # Invalid or no real clusters\n",
        "        row_scores.append(score)\n",
        "    heatmap_scores.append(row_scores)\n",
        "\n",
        "# Convert to DataFrame\n",
        "heatmap_df = pd.DataFrame(heatmap_scores, index=min_samples_values, columns=eps_values)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(heatmap_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"DBSCAN Silhouette Score Heatmap\")\n",
        "plt.xlabel(\"eps values\")\n",
        "plt.ylabel(\"min_samples values\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Define parameter grids\n",
        "eps_values = [0.5, 1.0, 1.5, 2.0]\n",
        "min_samples_values = [3, 5, 7]\n",
        "\n",
        "# Track best params and score\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "\n",
        "# Grid search for best DBSCAN params\n",
        "for eps, min_samples in product(eps_values, min_samples_values):\n",
        "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = db.fit_predict(rfm_scaled_df)\n",
        "\n",
        "    # Only consider valid clusters (no noise and more than one cluster)\n",
        "    if len(set(labels)) > 1 and -1 not in labels:\n",
        "        score = silhouette_score(rfm_scaled_df, labels)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = {\"eps\": eps, \"min_samples\": min_samples}\n",
        "\n",
        "# Output best combination\n",
        "print(\"Best Params:\", best_params)\n",
        "print(\"Improved Silhouette Score:\", round(best_score, 3))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual grid search using combinations of eps and min_samples, since DBSCAN is sensitive to these and visual intuition helps."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, tuning eps and min_samples improved the cluster quality and reduced noise, giving a better silhouette score.\n",
        "\n",
        "**Silhouette Score:** Helps assess how well DBSCAN clusters dense areas while ignoring noise.\n",
        "\n",
        "**Business Impact**: Useful for identifying outliers (one-time buyers or frauds) and understanding dense clusters (loyal or frequent buyers).\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We considered the following metrics for evaluating clustering performance:\n",
        "\n",
        "1. Silhouette Score\n",
        "Measures how well-separated the clusters are.\n",
        "\n",
        "A higher score indicates that the clusters are dense and well-separated, which is ideal for actionable segmentation.\n",
        "\n",
        "Business impact: Helps ensure clear and interpretable customer segments, crucial for personalized marketing and customer strategy.\n",
        "\n",
        "2. Davies-Bouldin Index (optional)\n",
        "Measures intra-cluster similarity and inter-cluster differences (lower is better).\n",
        "\n",
        "Can be used to double-check silhouette findings.\n",
        "\n",
        "Business impact: Lower values help ensure less overlap among customer segments, reducing marketing budget leakage.\n",
        "\n",
        "**Silhouette Score was prioritized as it's intuitive and interpretable in a customer segmentation context.**"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose ML Model 1: KMeans Clustering as the final model.\n",
        "\n",
        "Reasons:\n",
        "Achieved highest silhouette score (~0.41) after tuning.\n",
        "\n",
        "Produced balanced and well-separated clusters.\n",
        "\n",
        "KMeans is computationally efficient and works well with standardized numerical features like Recency, Frequency, and Monetary (RFM) metrics.\n",
        "\n",
        "Resulting clusters were easy to interpret and actionable for the business.\n",
        "\n",
        "**KMeans gave the most stable and explainable clustering, ideal for creating targeted business strategies.**"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used: KMeans Clustering\n",
        "It’s an unsupervised learning algorithm that partitions data into k distinct clusters based on feature similarity.\n",
        "\n",
        "Each data point is assigned to the cluster with the nearest mean.\n",
        "\n",
        "**Feature Importance with SHAP (Model Explainability Tool)**\n",
        "Although SHAP is typically used for supervised models, we can estimate feature influence using:"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Fit KMeans model\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(rfm_scaled_df)\n",
        "\n",
        "# Add cluster labels to your DataFrame for reference (optional)\n",
        "rfm_scaled_df['Cluster'] = cluster_labels\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "import pandas as pd\n",
        "\n",
        "# Define features and target\n",
        "X = rfm_scaled_df.drop('Cluster', axis=1)\n",
        "y = rfm_scaled_df['Cluster']\n",
        "\n",
        "# Fit a Random Forest model (supervised)\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Compute permutation importance\n",
        "perm = permutation_importance(rf, X, y, n_repeats=10, random_state=42)\n",
        "\n",
        "# Display feature importances\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': perm.importances_mean\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(importance_df)"
      ],
      "metadata": {
        "id": "q1wFwRq02RKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Interpretation:**\n",
        "Monetary and Frequency features were most influential in determining clusters.\n",
        "\n",
        "This insight helps Myntra prioritize high-value frequent shoppers for loyalty or retention campaigns.\n",
        "\n",
        "**Business Impact:** Feature importance helps identify which customer traits matter most — guiding personalized offers, stock prioritization, and communication tone for each customer group."
      ],
      "metadata": {
        "id": "nQqB3wGyfmI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save model\n",
        "joblib.dump(kmeans, 'kmeans_model.joblib')\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model and scaler\n",
        "# Step 1: Load the saved model\n",
        "loaded_model = joblib.load('kmeans_model.joblib')\n",
        "\n",
        "# Step 2: Prepare unseen data (example row from RFM scaled dataset)\n",
        "# Let's say we take the first 5 rows of unseen data as a simulation\n",
        "unseen_data = rfm_scaled_df.iloc[:5]\n",
        "\n",
        "# Step 3: Predict using the loaded model\n",
        "predicted_clusters = loaded_model.predict(unseen_data)\n",
        "\n",
        "# Step 4: Display results\n",
        "print(\"Predicted Cluster Labels:\", predicted_clusters)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully implemented an end-to-end customer segmentation solution for Myntra using unsupervised machine learning techniques. Our approach began with a deep exploration of customer purchase data through descriptive statistics, visualization, and hypothesis testing, allowing us to extract key behavioral insights.\n",
        "\n",
        "We applied feature engineering, handled missing values and outliers, and carefully prepared the dataset through scaling and dimensionality reduction. This ensured that the models received clean, meaningful input.\n",
        "\n",
        "We then implemented and evaluated three clustering algorithms — KMeans, Hierarchical Clustering, and DBSCAN. Using metrics like the Silhouette Score and Davies-Bouldin Index, we compared model performances. Among these, KMeans provided the most stable and interpretable clusters.\n",
        "\n",
        "Hyperparameter tuning and model validation further improved the clustering performance, helping us derive well-separated customer segments that can be used to:\n",
        "\n",
        "-Identify high-value customers\n",
        "\n",
        "-Target specific customer groups with personalized marketing\n",
        "\n",
        "-Improve customer retention and engagement\n",
        "\n",
        "-Support seasonal campaign planning based on spending trends\n",
        "\n",
        "We also incorporated hypothesis testing to validate assumptions around regional spending patterns, seasonal behavior, and product pricing sensitivity, providing strong statistical backing for strategic business decisions.\n",
        "\n",
        "Finally, the best model was saved and tested for real-time deployment, ensuring scalability in a production environment.\n",
        "\n",
        "###  **Business Impact**\n",
        "\n",
        "The customer segmentation solution developed in this project has significant implications for Myntra's business growth and marketing efficiency. By leveraging unsupervised machine learning to group customers based on purchase patterns and behavior, Myntra can now:\n",
        "\n",
        "1. **Personalize Marketing Campaigns**  \n",
        "   - Each customer segment can be targeted with tailored promotions, discounts, and product recommendations, leading to increased **click-through rates**, **conversion rates**, and **customer satisfaction**.\n",
        "\n",
        "2. **Improve Customer Retention**  \n",
        "   - By identifying **loyal, high-value customers**, Myntra can invest in exclusive rewards and retention strategies to increase **lifetime value** and reduce churn.\n",
        "\n",
        "3. **Optimize Inventory and Product Planning**  \n",
        "   - Segment-based demand forecasting enables better planning of stock levels, especially during peak seasons, minimizing **overstocking** or **stockouts**.\n",
        "\n",
        "4. **Strategize Seasonal Campaigns**  \n",
        "   - Insights from hypothesis testing, such as increased spending during the holiday season, help allocate marketing budgets more effectively to **maximize festive revenue**.\n",
        "\n",
        "5. **Enhance Customer Experience**  \n",
        "   - Understanding behavior at a granular level allows Myntra to improve UX, recommend relevant products, and streamline user journeys — all driving better **engagement**.\n",
        "\n",
        "6. **Enable Data-Driven Decision Making**  \n",
        "   - The clusters and insights generated can be integrated into dashboards and CRM systems, empowering teams across sales, marketing, and product with **actionable intelligence**."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}